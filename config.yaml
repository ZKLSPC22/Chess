# This file is the single source of truth for all configurations.

# Default configurations for various components.
# Agents will inherit from these, and they can be overridden
# in the 'agent_settings' section below.
default_configs:
  ppo:
    model:
      channels: 128
      num_res_blocks: 10
      activation: 'relu'
    train:
      learning_rate: 0.001
      optimizer: 'adam'
      epochs: 10
      batch_size: 64
      clip_eps: 0.2
      value_coef: 0.5
      entropy_coef: 0.01
  
  ppo_data_collection:
    count: 2048
    gamma: 0.99

  policy_value_train:
    epochs: 5
    batch_size: 128
    value_coef: 1.0
    weight_decay: 0.0001

# Agent-specific settings.
# Here you can define different agents that override parts of the default configs.
agent_settings:
  ppo_resnet:
    # This agent uses the defaults defined above without changes.
    # If you wanted to, you could override a specific value like this:
    # ppo:
    #   train:
    #     learning_rate: 0.0005
    pass

  mcts_ppo_pvl_resnet:
    mcts:
      num_simulations: 25
    ppo:
      train:
        # This agent uses fewer epochs for its PPO training than the default.
        epochs: 5

# Logging Configuration
logging:
  application:
    level: INFO # General log level. Can be DEBUG, INFO, WARNING, ERROR, CRITICAL
    file: logs/app.log
    console: true # Show general logs in the console
  training:
    level: DEBUG # Log detailed training metrics
    file: logs/training.log
    console: false
  evaluation:
    level: INFO # Log game outcomes
    file: logs/evaluation.log
    console: false
  